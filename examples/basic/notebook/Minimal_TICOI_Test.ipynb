{
 "cells": [
  {
   "cell_type": "code",
   "id": "52dcd129-4448-4023-92d4-1b92a2c8f9c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:01:39.126550Z",
     "start_time": "2025-05-07T14:01:39.124026Z"
    }
   },
   "source": [
    "db = '/media/ttsmith/DB/Dropbox/'\n",
    "tmp_dir = db + 'TMP/'\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# zarr_gdf = gpd.read_file(db + 'Glacier Velocity/ITS_LIVE/catalog_v02.json')\n",
    "zarr_gdf = \"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:01:41.459239Z",
     "start_time": "2025-05-07T14:01:41.448964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_loc(zarr_gdf, xy):\n",
    "    pt_loc = Point(xy)\n",
    "    xy_proj = gpd.GeoDataFrame(geometry=[pt_loc],crs='epsg:4326')\n",
    "    print(xy_proj)\n",
    "    subset = gpd.sjoin(xy_proj, zarr_gdf, how=\"inner\", predicate='within')\n",
    "    return subset.zarr_url.values[0]\n",
    "\n",
    "def pull_xy(ds, xy): #SINGLE LOCATION\n",
    "    xy = Point(xy)\n",
    "    xy_proj = gpd.GeoDataFrame(geometry=[xy],crs='epsg:4326').to_crs(ds.projection)\n",
    "    data = ds.sel(x=xy_proj.geometry.x.values[0],y=xy_proj.geometry.y.values[0],method='nearest')\n",
    "    velocity = data.v.values\n",
    "    timing = np.array([pd.Timestamp(x) for x in data.date_center.values])\n",
    "    error = data.v_error.values\n",
    "    dt = data.date_dt.values\n",
    "\n",
    "    d0 = np.array([pd.Timestamp(x) for x in data.acquisition_date_img1.values])\n",
    "    d1 = np.array([pd.Timestamp(x) for x in data.acquisition_date_img2.values])\n",
    "\n",
    "    days = np.array([convert_to_days(x) for x in dt])\n",
    "\n",
    "    #Remove locked (less than 1) pixels\n",
    "    velocity[velocity <= 0.5] = np.nan\n",
    "\n",
    "    #Also filter for local large outliers (negative, missed points)\n",
    "    velocity[velocity < np.nanpercentile(velocity, 2)] = np.nan\n",
    "\n",
    "    idx = ~np.isnan(velocity)\n",
    "    velocity, timing, error, days = velocity[idx], timing[idx], error[idx], days[idx]\n",
    "    d0, d1 = d0[idx], d1[idx]\n",
    "\n",
    "    #Filter to Landsat-8 Era\n",
    "    idx = np.where(timing > pd.Timestamp(2013,4,10))\n",
    "    velocity, timing, error, days = velocity[idx], timing[idx], error[idx], days[idx]\n",
    "    d0, d1 = d0[idx], d1[idx]\n",
    "\n",
    "    return velocity, timing, error, days, d0, d1\n",
    "\n",
    "def convert_to_days(x):\n",
    "    x = np.timedelta64(x, 'ns')\n",
    "    days = x.astype('timedelta64[D]')\n",
    "    return days / np.timedelta64(1, 'D')\n",
    "\n",
    "def create_consensus_series(midpoints, v_obs, v_uncert, time_spacing, n_samp):\n",
    "    def create_uniform_dates(start_year, end_year, time_spacing=16):\n",
    "        #Define uniform spacing for whole period\n",
    "        uniform_times = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            year_start = pd.Timestamp(str(year) + '-01-01')\n",
    "            year_days = 366 if pd.Timestamp(str(year) + '-12-31').is_leap_year else 365\n",
    "            steps = np.arange(0, year_days, time_spacing)  #X-day intervals\n",
    "            uniform_times.extend((year_start + pd.to_timedelta(steps, unit=\"D\")).tolist())\n",
    "\n",
    "        uniform_times = np.array(uniform_times)\n",
    "\n",
    "        #Convert uniform times into linear days distance\n",
    "        reference_date = pd.Timestamp(str(start_year) + '-01-01')\n",
    "        uniform_times_numeric = np.array([(t - reference_date).days for t in uniform_times])\n",
    "        n_uniform = len(uniform_times_numeric) - 1  #Number of steps to invert onto\n",
    "\n",
    "        return uniform_times, uniform_times_numeric, n_uniform, reference_date\n",
    "\n",
    "    uniform_times, uniform_times_numeric, n_uniform, reference_date = create_uniform_dates(midpoints.index.year.min(), midpoints.index.year.max(), time_spacing)\n",
    "    midpoints_numeric = (midpoints - reference_date).dt.days\n",
    "\n",
    "    #Get which bin each midpoint maps to\n",
    "    inversion_dates = np.array([np.argmin(np.abs(uniform_times_numeric - mid)) for mid in midpoints_numeric])\n",
    "    unique_dates, ct = np.unique(inversion_dates, return_counts=True)\n",
    "\n",
    "    #Choose a min number of points per bin\n",
    "    sample_size = int(np.nanpercentile(ct, 5))\n",
    "\n",
    "    def get_vel_series(inversion_dates, unique_dates, v_obs, v_uncert, n=100):\n",
    "        def get_series(inversion_dates, unique_dates, v_obs, v_uncert):\n",
    "            #For each bin, choose a random sample of velocities and their weights\n",
    "            avgd = []\n",
    "            for unique_id in unique_dates:\n",
    "                idx = np.where(inversion_dates == unique_id)\n",
    "                v, ve = v_obs[idx], v_uncert[idx]\n",
    "                rsub = np.random.randint(0, high=v.shape[0], size=sample_size)\n",
    "                vc, vec = v[rsub], ve[rsub]\n",
    "                avg = np.average(vc, weights=vec)\n",
    "                avgd.append(avg)\n",
    "            return avgd\n",
    "\n",
    "        emp = np.empty((unique_dates.shape[0], n))\n",
    "        for i in range(n):\n",
    "            avgd = get_series(inversion_dates, unique_dates, v_obs, v_uncert)\n",
    "            emp[:,i] = avgd\n",
    "        return emp\n",
    "\n",
    "    averages = get_vel_series(inversion_dates, unique_dates, v_obs, v_uncert, n=n_samp)\n",
    "    consensus = np.nanmean(averages, axis=1)\n",
    "    f_ser = pd.Series(consensus, index=uniform_times[unique_dates])\n",
    "    return f_ser, averages, uniform_times[unique_dates]"
   ],
   "id": "67e6abad728242c3",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "cb46055e-19e7-402e-b083-e0d45a1dc3a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:02:05.309501Z",
     "start_time": "2025-05-07T14:02:05.285365Z"
    }
   },
   "source": [
    "#MY VERSION\n",
    "lon = -140.84746241412898\n",
    "lat = 60.08936753843976\n",
    "print(choose_loc(zarr_gdf, [lon,lat]))\n",
    "ds = xr.open_dataset(choose_loc(zarr_gdf, [lon,lat]),engine=\"zarr\")\n",
    "velocity, timing, error, days, d0, d1 = pull_xy(ds, [lon,lat])\n",
    "timing = [pd.Timestamp(x) for x in timing]\n",
    "midpoints = pd.Series(timing,index=timing) #Midpoints\n",
    "\n",
    "time_spacing = 4\n",
    "f_ser, sample_series, series_index = create_consensus_series(midpoints, velocity, error, time_spacing=time_spacing, n_samp=100)\n",
    "f_ser.plot()\n",
    "\n",
    "time_spacing = 16\n",
    "f_ser, sample_series, series_index = create_consensus_series(midpoints, velocity, error, time_spacing=time_spacing, n_samp=100)\n",
    "f_ser.plot()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      geometry\n",
      "0  POINT (-140.84746 60.08937)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'right_df' should be GeoDataFrame, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m lon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m140.84746241412898\u001B[39m\n\u001B[1;32m      3\u001B[0m lat \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m60.08936753843976\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mchoose_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mzarr_gdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mlon\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlat\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      5\u001B[0m ds \u001B[38;5;241m=\u001B[39m xr\u001B[38;5;241m.\u001B[39mopen_dataset(choose_loc(zarr_gdf, [lon,lat]),engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzarr\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m velocity, timing, error, days, d0, d1 \u001B[38;5;241m=\u001B[39m pull_xy(ds, [lon,lat])\n",
      "Cell \u001B[0;32mIn[18], line 5\u001B[0m, in \u001B[0;36mchoose_loc\u001B[0;34m(zarr_gdf, xy)\u001B[0m\n\u001B[1;32m      3\u001B[0m xy_proj \u001B[38;5;241m=\u001B[39m gpd\u001B[38;5;241m.\u001B[39mGeoDataFrame(geometry\u001B[38;5;241m=\u001B[39m[pt_loc],crs\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepsg:4326\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(xy_proj)\n\u001B[0;32m----> 5\u001B[0m subset \u001B[38;5;241m=\u001B[39m \u001B[43mgpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxy_proj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mzarr_gdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwithin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m subset\u001B[38;5;241m.\u001B[39mzarr_url\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/env_ticoi2/lib/python3.11/site-packages/geopandas/tools/sjoin.py:114\u001B[0m, in \u001B[0;36msjoin\u001B[0;34m(left_df, right_df, how, predicate, lsuffix, rsuffix, distance, on_attribute, **kwargs)\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msjoin() got an unexpected keyword argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfirst\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    112\u001B[0m on_attribute \u001B[38;5;241m=\u001B[39m _maybe_make_list(on_attribute)\n\u001B[0;32m--> 114\u001B[0m \u001B[43m_basic_checks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mleft_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mright_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlsuffix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrsuffix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon_attribute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_attribute\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    116\u001B[0m indices \u001B[38;5;241m=\u001B[39m _geom_predicate_query(\n\u001B[1;32m    117\u001B[0m     left_df, right_df, predicate, distance, on_attribute\u001B[38;5;241m=\u001B[39mon_attribute\n\u001B[1;32m    118\u001B[0m )\n\u001B[1;32m    120\u001B[0m joined, _ \u001B[38;5;241m=\u001B[39m _frame_join(\n\u001B[1;32m    121\u001B[0m     left_df,\n\u001B[1;32m    122\u001B[0m     right_df,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    129\u001B[0m     on_attribute\u001B[38;5;241m=\u001B[39mon_attribute,\n\u001B[1;32m    130\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/env_ticoi2/lib/python3.11/site-packages/geopandas/tools/sjoin.py:169\u001B[0m, in \u001B[0;36m_basic_checks\u001B[0;34m(left_df, right_df, how, lsuffix, rsuffix, on_attribute)\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft_df\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should be GeoDataFrame, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(left_df))\n\u001B[1;32m    166\u001B[0m     )\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(right_df, GeoDataFrame):\n\u001B[0;32m--> 169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright_df\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should be GeoDataFrame, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(right_df))\n\u001B[1;32m    171\u001B[0m     )\n\u001B[1;32m    173\u001B[0m allowed_hows \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m allowed_hows:\n",
      "\u001B[0;31mValueError\u001B[0m: 'right_df' should be GeoDataFrame, got <class 'str'>"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91c4b-e4d7-4283-bd59-004573a18d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Version\n",
    "from src.ticoi.core import ticoi_one_pixel\n",
    "lon = -140.84746241412898\n",
    "lat = 60.08936753843976\n",
    "pick_date = [\"2013-01-01\", \"2025-01-01\"]# date range to study\n",
    "coef = 100  # Regularization coefficient to be used\n",
    "delete_outliers = {\"median_angle\": 45} #Remove the observation if its direction is 45° away from the direction of the median vector\n",
    "save = True  # Save the results and figures\n",
    "show = True  # Plot some figures\n",
    "option_visual = [\"obs_magnitude\", \"invertvv_overlaid\", \"quality_metrics\"] #check README_visualization_pixel_output to see the different options .\n",
    "result_quality = [\n",
    "    \"Error_propagation\",\n",
    "    \"X_contribution\",\n",
    "]  # Criterium used to evaluate the quality of the results: (\"Error_propagation\": the initial error given in the dataset is propagated through the inversion; \"X_contribution\" correspond to the number of observed velocity used to estimate each estimated value\n",
    "\n",
    "url_ls = choose_loc(zarr_gdf,[lon,lat])\n",
    "data,dataf,dataf_lp = ticoi_one_pixel(cube_name=url_ls,i=lon,j=lat,path_save=tmp_dir,save=save,show=show,option_visual=option_visual,load_kwargs={\"pick_date\":pick_date,\"buffer\": [lon, lat, 0.1]},load_pixel_kwargs={\"visual\":show},preData_kwargs={\"delete_outliers\":delete_outliers},inversion_kwargs = {\"coef\":coef,\"result_quality\":result_quality,\"visual\":show},interpolation_kwargs = {\"result_quality\":result_quality})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb487d-02cc-43fa-abdf-175401c5b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer version, fails in data loading somewhere\n",
    "lon = -140.84746241412898\n",
    "lat = 60.08936753843976\n",
    "url_ls = choose_loc(zarr_gdf,[lon,lat])\n",
    "\n",
    "import time\n",
    "from src.ticoi.cube_data_classxr import CubeDataClass\n",
    "\n",
    "pick_date = [\"2013-01-01\", \"2025-01-01\"]# date range to study\n",
    "coef = 100  # Regularization coefficient to be used\n",
    "delete_outliers = {\"median_angle\": 45} #Remove the observation if its direction is 45° away from the direction of the median vector\n",
    "\n",
    "apriori_weight = False  # Use the error as apriori\n",
    "interval_output = 30  # temporal sampling of the output results\n",
    "unit = 365  # 1 for m/d, 365 for m/y\n",
    "regu = \"1accelnotnull\"  # Regularization method.s to be used (for each flag if flags is not None) : 1 minimize the acceleration, '1accelnotnull' minize the distance with an apriori on the acceleration computed over a spatio-temporal filtering of the cube\n",
    "result_quality = [\n",
    "    \"Error_propagation\",\n",
    "    \"X_contribution\",\n",
    "]  # Criterium used to evaluate the quality of the results ('X_count', 'Norm_residual', 'X_contribution', 'Error_propagation')\n",
    "verbose = True  # Print information throughout TICOI processing\n",
    "\n",
    "proj = \"EPSG:4326\"\n",
    "load_kwargs = {\n",
    "    \"chunks\": {},\n",
    "    \"conf\": False,  # If True, confidence indicators will be put between 0 and 1, with 1 the lowest errors\n",
    "    \"subset\": None,  # Subset of the data to be loaded ([xmin, xmax, ymin, ymax] or None)\n",
    "    \"buffer\": None, # Area to be loaded around the pixel ([longitude, latitude, buffer size] or None)\n",
    "    \"nearest\": [lon, lat], #Loc for nearest sample\n",
    "    \"pick_date\": pick_date,  # Select dates ([min, max] or None to select all)\n",
    "    \"pick_sensor\": None,  # Select sensors (None to select all)\n",
    "    \"pick_temp_bas\": None,  # Select temporal baselines ([min, max] in days or None to select all)\n",
    "    \"proj\": proj,  # EPSG system of the given coordinates\n",
    "    \"verbose\": verbose,  # Print information throughout the loading process\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "cube = CubeDataClass()\n",
    "cube.load(url_ls, **load_kwargs)\n",
    "print(f\"[Data loading] Cube of dimension (nz,nx,ny) : ({cube.nz}, {cube.nx}, {cube.ny}) \")\n",
    "stop = time.time()\n",
    "print(f\"[Data loading] Loading the Zarr took {round((stop - start), 4)} s\")\n",
    "\n",
    "proj = \"EPSG:3413\"  # EPSG system of the given coordinates\n",
    "preData_kwargs = {\n",
    "    \"smooth_method\": \"savgol\",  # Smoothing method to be used to smooth the data in time ('gaussian', 'median', 'emwa', 'savgol')\n",
    "    \"s_win\": 3,  # Size of the spatial window\n",
    "    \"t_win\": 90,  # Time window size for 'ewma' smoothing\n",
    "    \"sigma\": 3,  # Standard deviation for 'gaussian' filter\n",
    "    \"order\": 3,  # Order of the smoothing function\n",
    "    \"unit\": 365,  # 365 if the unit is m/y, 1 if the unit is m/d\n",
    "    \"delete_outliers\": delete_outliers,  # Delete the outliers from the data according to one (int or str) or several (dict) criteriums\n",
    "    \"flag\": None,  # Divide the data in several areas where different methods should be used\n",
    "    \"dem_file\": None,  # Path to the DEM file for calculating the slope and aspect\n",
    "    \"regu\": regu,  # Regularization method.s to be used (for each flag if flags is not None) : 1 minimize the acceleration, '1accelnotnull' minize the distance with an apriori on the acceleration computed over a spatio-temporal filtering of the cube\n",
    "    \"solver\": \"LSMR_ini\",  # Solver for the inversion\n",
    "    \"proj\": proj,  # EPSG system of the given coordinates\n",
    "    \"velo_or_disp\": \"velo\",  # Type of data contained in the data cube ('disp' for displacements, and 'velo' for velocities)\n",
    "    \"verbose\": True,  # Print information throughout the filtering process\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "# Filter the cube (compute rolling_mean for regu=1accelnotnull)\n",
    "obs_filt, flag = cube.filter_cube_before_inversion(**preData_kwargs)\n",
    "stop = time.time()\n",
    "print(f\"Preprocess Filtering took {round((stop - start), 4)} s\")\n",
    "\n",
    "show = True  # Plot some figures\n",
    "save = True  # Save the results and figures\n",
    "option_visual = [\"obs_magnitude\", \"invertvv_overlaid\", \"quality_metrics\"] #check README_visualization_pixel_output to see the different options .\n",
    "path_save = save_dir + 'TICOI/'\n",
    "load_pixel_kwargs = {\n",
    "    \"regu\": regu,  # Regularization method to be used\n",
    "    \"coef\": coef,  # Regularization coefficient to be used\n",
    "    \"solver\": \"LSMR_ini\",  # Solver for the inversion\n",
    "    \"proj\": 'EPSG:4326',  # EPSG system of the given coordinates\n",
    "    \"interp\": \"nearest\",  # Interpolation method used to load the pixel when it is not in the dataset\n",
    "    \"visual\": show | save,  # If the observations data need to be returned\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "# Load pixel data\n",
    "data, mean, dates_range = cube.load_pixel(lon, lat, rolling_mean=obs_filt, **load_pixel_kwargs)\n",
    "\n",
    "# Prepare interpolation dates\n",
    "first_date_interpol, last_date_interpol = cube.prepare_interpolation_date()\n",
    "interpolation_kwargs.update({\"first_date_interpol\": first_date_interpol, \"last_date_interpol\": last_date_interpol})\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Prepping Interpolation took {round((stop - start), 4)} s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
